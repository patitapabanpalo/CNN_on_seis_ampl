{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "import os\n",
    "import numpy\n",
    "import time\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of training dataset and its labels\n",
      "torch.Size([20000, 9, 9])\n",
      "torch.Size([20000])\n",
      "the size of testing dataset and its labels\n",
      "torch.Size([6000, 9, 9])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "#the following codes reads the csv data files and converts them to trainloader and testloader\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "TrainList = []\n",
    "LabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Point\\Training\\Fault',\"9\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 9 and len(your_list[0]) == 9):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Point\\Training\\Non-Fault',\"9\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 9 and len(your_list[0]) == 9):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "inputs = torch.FloatTensor(TrainList)\n",
    "#labellist = [0,0,1,1]\n",
    "labels = torch.tensor(LabelList, dtype=torch.long)\n",
    "train_data = []\n",
    "for i in range(len(inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    train_data.append([inputs[i], labels[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=10)\n",
    "print('the size of training dataset and its labels')\n",
    "print(inputs.shape);\n",
    "print(labels.shape);\n",
    "\n",
    "TestList = []\n",
    "OLabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Point\\Testing\\Fault',\"9\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 9 and len(your_list[0]) == 9):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Point\\Testing\\Non-Fault',\"9\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 9 and len(your_list[0]) == 9):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "test_inputs = torch.FloatTensor(TestList)\n",
    "#labellist = [0,0,1,1]\n",
    "test_labels = torch.tensor(OLabelList, dtype=torch.long)\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(test_inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    test_data.append([test_inputs[i], test_labels[i]])\n",
    "    \n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=10)\n",
    "print('the size of testing dataset and its labels')\n",
    "print(test_inputs.shape);\n",
    "print(test_labels.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two calsses of data for the labels 0 and 1 respectively \n",
    "\n",
    "classes = ('non_fault', 'fault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22f33a35488>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an seismic data\n",
    "print('three random examples of training fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[1200], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[1600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[8400], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training non-fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22f33b5bbc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('three random examples of training non-fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[10800], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[14300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[17700], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four random examples of testing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22f33c90248>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('four random examples of testing data')\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(test_inputs[450], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(test_inputs[2300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(test_inputs[4600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(test_inputs[5800], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "        self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(81, 144)\n",
    "        self.fc2 = nn.Linear(144, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "#optimizer = optim.Adadelta(net.parameters(), lr=0.0001, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "#optimizer = optim.AdamW(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "#optimizer = optim.ASGD(net.parameters(), lr=0.0001, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#optimizer = optim.Rprop(net.parameters(), lr=0.0001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   600] loss: 0.693\n",
      "[1,  1200] loss: 0.691\n",
      "[1,  1800] loss: 0.691\n",
      "3.915738582611084 seconds\n",
      "[2,   600] loss: 0.689\n",
      "[2,  1200] loss: 0.687\n",
      "[2,  1800] loss: 0.687\n",
      "3.8888566493988037 seconds\n",
      "[3,   600] loss: 0.685\n",
      "[3,  1200] loss: 0.686\n",
      "[3,  1800] loss: 0.682\n",
      "3.884875774383545 seconds\n",
      "[4,   600] loss: 0.683\n",
      "[4,  1200] loss: 0.680\n",
      "[4,  1800] loss: 0.680\n",
      "4.508129835128784 seconds\n",
      "[5,   600] loss: 0.678\n",
      "[5,  1200] loss: 0.679\n",
      "[5,  1800] loss: 0.677\n",
      "6.2753355503082275 seconds\n",
      "[6,   600] loss: 0.676\n",
      "[6,  1200] loss: 0.676\n",
      "[6,  1800] loss: 0.673\n",
      "6.21560001373291 seconds\n",
      "[7,   600] loss: 0.672\n",
      "[7,  1200] loss: 0.672\n",
      "[7,  1800] loss: 0.671\n",
      "6.19369649887085 seconds\n",
      "[8,   600] loss: 0.669\n",
      "[8,  1200] loss: 0.669\n",
      "[8,  1800] loss: 0.667\n",
      "6.268366813659668 seconds\n",
      "[9,   600] loss: 0.665\n",
      "[9,  1200] loss: 0.665\n",
      "[9,  1800] loss: 0.664\n",
      "6.3241212368011475 seconds\n",
      "[10,   600] loss: 0.662\n",
      "[10,  1200] loss: 0.661\n",
      "[10,  1800] loss: 0.661\n",
      "6.23451566696167 seconds\n",
      "[11,   600] loss: 0.660\n",
      "[11,  1200] loss: 0.657\n",
      "[11,  1800] loss: 0.656\n",
      "6.3340771198272705 seconds\n",
      "[12,   600] loss: 0.655\n",
      "[12,  1200] loss: 0.655\n",
      "[12,  1800] loss: 0.652\n",
      "6.349013090133667 seconds\n",
      "[13,   600] loss: 0.653\n",
      "[13,  1200] loss: 0.650\n",
      "[13,  1800] loss: 0.650\n",
      "6.330095291137695 seconds\n",
      "[14,   600] loss: 0.649\n",
      "[14,  1200] loss: 0.647\n",
      "[14,  1800] loss: 0.644\n",
      "6.372906684875488 seconds\n",
      "[15,   600] loss: 0.644\n",
      "[15,  1200] loss: 0.643\n",
      "[15,  1800] loss: 0.642\n",
      "6.398791074752808 seconds\n",
      "[16,   600] loss: 0.640\n",
      "[16,  1200] loss: 0.638\n",
      "[16,  1800] loss: 0.639\n",
      "6.368923187255859 seconds\n",
      "[17,   600] loss: 0.636\n",
      "[17,  1200] loss: 0.637\n",
      "[17,  1800] loss: 0.632\n",
      "6.285292387008667 seconds\n",
      "[18,   600] loss: 0.632\n",
      "[18,  1200] loss: 0.631\n",
      "[18,  1800] loss: 0.630\n",
      "6.2534339427948 seconds\n",
      "[19,   600] loss: 0.627\n",
      "[19,  1200] loss: 0.628\n",
      "[19,  1800] loss: 0.627\n",
      "6.092143774032593 seconds\n",
      "[20,   600] loss: 0.624\n",
      "[20,  1200] loss: 0.622\n",
      "[20,  1800] loss: 0.622\n",
      "6.121017217636108 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#train the network for 20 epochs\n",
    "\n",
    "for epoch in range(20): # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print('{} seconds'.format(time.time() - t0))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the network parameters\n",
    "\n",
    "PATH = './params/seismic_net_9.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted non fault and fault probabilities\n",
      "tensor([[0.5345, 0.4655],\n",
      "        [0.5281, 0.4719],\n",
      "        [0.4813, 0.5187],\n",
      "        [0.5226, 0.4774],\n",
      "        [0.5248, 0.4752],\n",
      "        [0.5230, 0.4770],\n",
      "        [0.4630, 0.5370],\n",
      "        [0.1927, 0.8073],\n",
      "        [0.3398, 0.6602],\n",
      "        [0.5124, 0.4876]], grad_fn=<SoftmaxBackward>)\n",
      "Predicted:  non_fault\n"
     ]
    }
   ],
   "source": [
    "#test sample data from testing\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "test_ips, labels = dataiter.next()\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(test_ips)\n",
    "print('predicted non fault and fault probabilities')\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test dataset: 87 %\n",
      "6000.0\n"
     ]
    }
   ],
   "source": [
    "#test the performance of the network on whole dataset\n",
    "\n",
    "correct = 0.00\n",
    "total = 0.00\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        seis_data, labels = data\n",
    "        outputs = net(seis_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test dataset: %d %%' % (\n",
    "    100 * correct / total))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "78.58\n"
     ]
    }
   ],
   "source": [
    "#this part runs the code on GPU 10 times and reports the average accuracy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(81, 144)\n",
    "            self.fc2 = nn.Linear(144, 32)\n",
    "            self.fc3 = nn.Linear(32, 8)\n",
    "            self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_9.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()[0].to(device), dataiter.next()[1].to(device)\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    net.to(device)\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "77.75\n"
     ]
    }
   ],
   "source": [
    "#this part is same code as above (runs 10 times and reports avg accuracy) but doesn't need GPU to run\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(81, 144)\n",
    "            self.fc2 = nn.Linear(144, 32)\n",
    "            self.fc3 = nn.Linear(32, 8)\n",
    "            self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_9.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
