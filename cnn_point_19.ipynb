{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "import os\n",
    "import numpy\n",
    "import time\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of training dataset and its labels\n",
      "torch.Size([20000, 19, 19])\n",
      "torch.Size([20000])\n",
      "the size of testing dataset and its labels\n",
      "torch.Size([6000, 19, 19])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "#the following codes reads the csv data files and converts them to trainloader and testloader\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "TrainList = []\n",
    "LabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Point\\Training\\Fault',\"19\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 19 and len(your_list[0]) == 19):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Point\\Training\\Non-Fault',\"19\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 19 and len(your_list[0]) == 19):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "inputs = torch.FloatTensor(TrainList)\n",
    "#labellist = [0,0,1,1]\n",
    "labels = torch.tensor(LabelList, dtype=torch.long)\n",
    "train_data = []\n",
    "for i in range(len(inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    train_data.append([inputs[i], labels[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=10)\n",
    "print('the size of training dataset and its labels')\n",
    "print(inputs.shape);\n",
    "print(labels.shape);\n",
    "\n",
    "TestList = []\n",
    "OLabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Point\\Testing\\Fault',\"19\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 19 and len(your_list[0]) == 19):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Point\\Testing\\Non-Fault',\"19\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 19 and len(your_list[0]) == 19):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "test_inputs = torch.FloatTensor(TestList)\n",
    "#labellist = [0,0,1,1]\n",
    "test_labels = torch.tensor(OLabelList, dtype=torch.long)\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(test_inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    test_data.append([test_inputs[i], test_labels[i]])\n",
    "    \n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=10)\n",
    "print('the size of testing dataset and its labels')\n",
    "print(test_inputs.shape);\n",
    "print(test_labels.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two calsses of data for the labels 0 and 1 respectively \n",
    "\n",
    "classes = ('non_fault', 'fault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a48c0f788>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an seismic data\n",
    "print('three random examples of training fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[1200], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[1600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[8400], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training non-fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a48d33a08>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('three random examples of training non-fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[10800], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[14300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[17700], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four random examples of testing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a48e66bc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABqCAYAAAClIwp2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJRUlEQVR4nO3dX4xUZxnH8d9TCCQufwSBdgMVWoMxtHKhBCGxxmpV1ESaqEm5kaZtgJiqidFI4oXZcEOCXpioCVhx8aa1RG2pkpJCq3ABKUtiEzRSlkYo6QaW8m+hSrfN48WOy8xhlp2Zc857znvm+0nIzHv2DO8zP06fvD1z5qy5uwAA8bmj6AIAAJ2hgQNApGjgABApGjgARIoGDgCRooEDQKRSNXAzW2NmJ8xs0Mw2Z1UUxpBvfsg2P2QbjnV6HbiZTZH0uqQvSDor6aikde7+z+zK617kmx+yzQ/ZhjU1xWtXShp09zckycyekbRW0oT/UGZzXVqUYsp8fLL3QtEljDs2NHTB3eerzXznTZ/uS3p6whVa7/LlhuH1ukXB9MSuU6c2HnLH3rs/r6qa+HtH2UrSbDNfEKbIKJ2XdMXd1EG2hR67kTh26dL/j90GaRr4Qklv1o3PSvrU7V+ySNILKabMx8ATvym6hHG2Zcvp2tO28l3S06OBhx7Ks7SJPfdcw/DV0dHx5/cmdp03Z07D2Ib/lldVTczuKFtJWiDp5zlVVQXfu/m07WwLPXYjYbt3n262Pc05cGuy7ZbzMWa2wcwGzGxAuphiuq4zab712Q7fuBGorEpo+9i9EqCoimg7W47dzqVp4Gcl3V03XiTpreRO7r7D3Ve4+wppborpus6k+dZnO3968mQFbqPtY3d2sNKi13a2HLudS9PAj0paamb3mNk0SY9I2pNNWRD55ols80O2AXV8Dtzd3zOzJyXtkzRF0k53/0dmlXU58s0P2eaHbMNK8yGm3H2vpL0Z1YIE8s0P2eaHbMPhm5gAEKlUK/CqsC2fLrqErrV69axgcx0+HGwqIAhW4AAQKRo4AESKBg4AkeIceFUsXCht3VrM3KtWNQxX9vffHKxZc9t9n/pYTjU1cd994eYCQmAFDgCRooEDQKQCn0K5JOlPYadsybKiC+gemzY1DP87/PWCCmnPoGbrq3qg6DJK7FDHr/z3HffqsRnPZlhLFTW7RxgrcACIFg0cACJFAweASNHAASBSNHAAiBQNHAAiRQMHgEgFvg58VGO/Mq9sThRdAEquV1e0QX8uuozS2lF0AV2KFTgARIoGDgCRooEDQKSCngPv1Tlt0M9CTtmSPm2afKeSO3b8P7KPHC9o9vUNo5kzvz/+fGTb0Yaf+R8abyf7iW3b8isr6ciRcHMBAbACB4BI0cABIFI0cACIFL9STVI5r02vBv/h7sSWVU33A9A+VuAAECkaOABEigYOAJHiHLik7SW6x8XGogtAU0P6oPr0uaLLKLGXO37l6Kg0NJRhKV2EFTgARGrSBm5mO83svJkdr9s218xeMrOTtcc5+ZZZXbsk/UBSX9028s3GY6dOacHAgO5/7bXxbWSblWOS/iJp//gWsg2vlVMo/ZJ+Iel3dds2Szrg7lvNbHNt/KPsy6u+1ZIelPTbxs3km4FH58/Xk3fdpW8NDtZvJttMLJZ0r8Ya+biOsr16dVgvvrg9lyqrbtIVuLsflHQxsXmtxhaPqj0+nHFdXeOjkj5w62byzcBnZs3S3ClTkpvJNhPzJE1LbiTbwDo9B36nuw9JUu1xwUQ7mtkGMxsws4F3OpysC7WUb3220tWgBUaso2NXuhGswIh1mO21YAVWTe4fYrr7Dndf4e4rmqw0kUJ9ttKsosupnMZ8pxddTqU0Zjuj6HKi1ellhOfMrNfdh8ysV9L5LItCJ/mOSHol77om8G7DaGTk5mdXtu2biX2T14vtV75OS/qG7Mh+1RoFx25+Osq2V2e0oQK3dM5T3wTbO12B79HNm0Cvl/R8h38PmiPf/JBtfsg2sElX4Gb2tKTPSppnZmcl/UTSVknPmtnjks5ISi6z0KKnNPYrla9p/OP6eSLfjDwq6ZCktzX2cTHZZudVScMa+7+vvZJMItvgJm3g7r5ugh99PuNautITifFG6YK7vy3yzUB/YjyDbDOzMjF+We7vkG1gQb9KP6SPq08vhJyyJdu1pOgSKmXmzJv/DY+MHL3NngDS4Kv0ABApGjgARIoGDgCR4naylTGqW6+xDqX69wJdqsv6lf5YdBml9e0Urx3SbPXpgcxqqabmt7xmBQ4AkaKBA0CkaOAAEKmg58B7eqZp+fLFIadsycbD+4ouoc6XOnrVcl3SPv0+41pa86/EvF/TdwqpA+g2rMABIFI0cACIFA0cACLFdeDI3MjIgbrRwcRPk7eIvp5zNUB1sQIHgEjRwAEgUkFPoVy/fk2HDx8KOWVLzL5YdAnj3IuuoH0PanfRJQBdiRU4AESKBg4AkaKBA0CkuIwQGUjeTrb+0sAziZ9dSIxPZ18OIrNI0k+LLqLkuJ0sAFQKDRwAIkUDB4BIcQ5ckvuviy4haq/ouw3jv9Y9/3Bi3yWJ8cEJzu3loS/Fa9+XdDmrQiro/aIL6FKswAEgUjRwAIgUDRwAImUe8OYbZjassQt/5+nWC4LLoAx1LXb3+e2+iGxb0lG20ni+11X8e5hI0fmmzZZj9/aa5hu0gY9Pajbg7iuCTzyJstbVjrK+h7LW1Y4yv4cy19aqsr6HstYlcQoFAKJFAweASBXVwHcUNO9kylpXO8r6HspaVzvK/B7KXFuryvoeylpXMefAAQDpcQoFACIVtIGb2RozO2Fmg2a2OeTcTWrZaWbnzex43ba5ZvaSmZ2sPc4pssZ2lSVfss29lkrlS7adC9bAzWyKpF9K+rKkZZLWmdmyUPM30S9pTWLbZkkH3H2ppAO1cRRKlm+/yDZP/apIvmSbTsgV+EpJg+7+hru/K+kZSWsDzt/A3Q9KupjYvFbSrtrzXZIeDlpUOqXJl2zzVbF8yTaFkA18oaQ368Zna9vK5E53H5Kk2uOCgutpR9nzJdt8xZov2aYQsoFbk21cApMd8s0P2eaHbFMI2cDPSrq7brxI0lsB52/FOTPrlaTa4/mC62lH2fMl23zFmi/ZphCygR+VtNTM7jGzaZIekbQn4Pyt2CNpfe35eknPF1hLu8qeL9nmK9Z8yTYNdw/2R9JXJL0u6ZSkH4ecu0ktT2vs16mPamwV8LikD2nsU+aTtce5RdYYa75kS75kG+YP38QEgEjxTUwAiBQNHAAiRQMHgEjRwAEgUjRwAIgUDRwAIkUDB4BI0cABIFL/A1G2QEfzNUhqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('four random examples of testing data')\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(test_inputs[450], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(test_inputs[2300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(test_inputs[4600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(test_inputs[5800], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "        self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(361, 480)\n",
    "        self.fc2 = nn.Linear(480, 256)\n",
    "        self.fc3 = nn.Linear(256, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "#optimizer = optim.Adadelta(net.parameters(), lr=0.0001, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "#optimizer = optim.AdamW(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "#optimizer = optim.ASGD(net.parameters(), lr=0.0001, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#optimizer = optim.Rprop(net.parameters(), lr=0.0001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   600] loss: 0.685\n",
      "[1,  1200] loss: 0.676\n",
      "[1,  1800] loss: 0.667\n",
      "12.850351810455322 seconds\n",
      "[2,   600] loss: 0.655\n",
      "[2,  1200] loss: 0.646\n",
      "[2,  1800] loss: 0.636\n",
      "12.723908424377441 seconds\n",
      "[3,   600] loss: 0.621\n",
      "[3,  1200] loss: 0.611\n",
      "[3,  1800] loss: 0.600\n",
      "12.778669118881226 seconds\n",
      "[4,   600] loss: 0.583\n",
      "[4,  1200] loss: 0.570\n",
      "[4,  1800] loss: 0.560\n",
      "12.730880498886108 seconds\n",
      "[5,   600] loss: 0.544\n",
      "[5,  1200] loss: 0.532\n",
      "[5,  1800] loss: 0.526\n",
      "12.595475196838379 seconds\n",
      "[6,   600] loss: 0.515\n",
      "[6,  1200] loss: 0.507\n",
      "[6,  1800] loss: 0.498\n",
      "12.680103063583374 seconds\n",
      "[7,   600] loss: 0.488\n",
      "[7,  1200] loss: 0.485\n",
      "[7,  1800] loss: 0.481\n",
      "12.530758619308472 seconds\n",
      "[8,   600] loss: 0.472\n",
      "[8,  1200] loss: 0.470\n",
      "[8,  1800] loss: 0.463\n",
      "12.572575569152832 seconds\n",
      "[9,   600] loss: 0.457\n",
      "[9,  1200] loss: 0.454\n",
      "[9,  1800] loss: 0.456\n",
      "12.527772903442383 seconds\n",
      "[10,   600] loss: 0.449\n",
      "[10,  1200] loss: 0.445\n",
      "[10,  1800] loss: 0.443\n",
      "12.451113224029541 seconds\n",
      "[11,   600] loss: 0.442\n",
      "[11,  1200] loss: 0.440\n",
      "[11,  1800] loss: 0.434\n",
      "12.553659915924072 seconds\n",
      "[12,   600] loss: 0.432\n",
      "[12,  1200] loss: 0.436\n",
      "[12,  1800] loss: 0.431\n",
      "12.615387916564941 seconds\n",
      "[13,   600] loss: 0.429\n",
      "[13,  1200] loss: 0.425\n",
      "[13,  1800] loss: 0.428\n",
      "12.467041015625 seconds\n",
      "[14,   600] loss: 0.420\n",
      "[14,  1200] loss: 0.424\n",
      "[14,  1800] loss: 0.421\n",
      "11.642677307128906 seconds\n",
      "[15,   600] loss: 0.419\n",
      "[15,  1200] loss: 0.418\n",
      "[15,  1800] loss: 0.418\n",
      "11.376847267150879 seconds\n",
      "[16,   600] loss: 0.417\n",
      "[16,  1200] loss: 0.415\n",
      "[16,  1800] loss: 0.410\n",
      "11.12196969985962 seconds\n",
      "[17,   600] loss: 0.410\n",
      "[17,  1200] loss: 0.411\n",
      "[17,  1800] loss: 0.411\n",
      "10.941766023635864 seconds\n",
      "[18,   600] loss: 0.408\n",
      "[18,  1200] loss: 0.412\n",
      "[18,  1800] loss: 0.405\n",
      "10.567415475845337 seconds\n",
      "[19,   600] loss: 0.407\n",
      "[19,  1200] loss: 0.409\n",
      "[19,  1800] loss: 0.405\n",
      "10.339421272277832 seconds\n",
      "[20,   600] loss: 0.402\n",
      "[20,  1200] loss: 0.405\n",
      "[20,  1800] loss: 0.403\n",
      "10.243842840194702 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#train the network for 20 epochs\n",
    "\n",
    "for epoch in range(20): # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print('{} seconds'.format(time.time() - t0))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the network parameters\n",
    "\n",
    "PATH = './params/seismic_net_19.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted non fault and fault probabilities\n",
      "tensor([[0.0067, 0.9933],\n",
      "        [0.9723, 0.0277],\n",
      "        [0.0126, 0.9874],\n",
      "        [0.0836, 0.9164],\n",
      "        [0.0085, 0.9915],\n",
      "        [0.0676, 0.9324],\n",
      "        [0.9816, 0.0184],\n",
      "        [0.9299, 0.0701],\n",
      "        [0.9781, 0.0219],\n",
      "        [0.9716, 0.0284]], grad_fn=<SoftmaxBackward>)\n",
      "Predicted:  fault\n"
     ]
    }
   ],
   "source": [
    "#test sample data from testing\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "test_ips, labels = dataiter.next()\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(test_ips)\n",
    "print('predicted non fault and fault probabilities')\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test dataset: 92 %\n",
      "6000.0\n"
     ]
    }
   ],
   "source": [
    "#test the performance of the network on whole dataset\n",
    "\n",
    "correct = 0.00\n",
    "total = 0.00\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        seis_data, labels = data\n",
    "        outputs = net(seis_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test dataset: %d %%' % (\n",
    "    100 * correct / total))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "92.01\n"
     ]
    }
   ],
   "source": [
    "#this part runs the code on GPU 10 times and reports the average accuracy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(361, 480)\n",
    "            self.fc2 = nn.Linear(480, 256)\n",
    "            self.fc3 = nn.Linear(256, 32)\n",
    "            self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_19.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()[0].to(device), dataiter.next()[1].to(device)\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    net.to(device)\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "91.97\n"
     ]
    }
   ],
   "source": [
    "#this part is same code as above (runs 10 times and reports avg accuracy) but doesn't need GPU to run\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(361, 480)\n",
    "            self.fc2 = nn.Linear(480, 256)\n",
    "            self.fc3 = nn.Linear(256, 32)\n",
    "            self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_19.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
