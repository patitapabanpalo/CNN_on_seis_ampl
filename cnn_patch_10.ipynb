{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "import os\n",
    "import numpy\n",
    "import time\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of training dataset and its labels\n",
      "torch.Size([20000, 10, 10])\n",
      "torch.Size([20000])\n",
      "the size of testing dataset and its labels\n",
      "torch.Size([6000, 10, 10])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "#the following codes reads the csv data files and converts them to trainloader and testloader\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "TrainList = []\n",
    "LabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Patch\\Training\\Fault',\"10\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 10 and len(your_list[0]) == 10):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Patch\\Training\\Non-Fault',\"10\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 10 and len(your_list[0]) == 10):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "inputs = torch.FloatTensor(TrainList)\n",
    "#labellist = [0,0,1,1]\n",
    "labels = torch.tensor(LabelList, dtype=torch.long)\n",
    "train_data = []\n",
    "for i in range(len(inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    train_data.append([inputs[i], labels[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=False, batch_size=10)\n",
    "print('the size of training dataset and its labels')\n",
    "print(inputs.shape);\n",
    "print(labels.shape);\n",
    "\n",
    "TestList = []\n",
    "OLabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Patch\\Testing\\Fault',\"10\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 10 and len(your_list[0]) == 10):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Patch\\Testing\\Non-Fault',\"10\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 10 and len(your_list[0]) == 10):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "test_inputs = torch.FloatTensor(TestList)\n",
    "#labellist = [0,0,1,1]\n",
    "test_labels = torch.tensor(OLabelList, dtype=torch.long)\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(test_inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    test_data.append([test_inputs[i], test_labels[i]])\n",
    "    \n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=10)\n",
    "print('the size of testing dataset and its labels')\n",
    "print(test_inputs.shape);\n",
    "print(test_labels.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two calsses of data for the labels 0 and 1 respectively \n",
    "\n",
    "classes = ('non_fault', 'fault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c0366ac7c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an seismic data\n",
    "print('three random examples of training fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[1200], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[1600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[8400], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training non-fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c0367d1e48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('three random examples of training non-fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[10800], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[14300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[17700], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four random examples of testing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c036908148>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('four random examples of testing data')\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(test_inputs[450], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(test_inputs[2300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(test_inputs[4600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(test_inputs[5800], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "        self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(100, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "#optimizer = optim.Adadelta(net.parameters(), lr=0.0001, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "#optimizer = optim.AdamW(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "#optimizer = optim.ASGD(net.parameters(), lr=0.0001, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#optimizer = optim.Rprop(net.parameters(), lr=0.0001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   600] loss: 0.582\n",
      "[1,  1200] loss: 0.657\n",
      "[1,  1800] loss: 0.822\n",
      "2.96492862701416 seconds\n",
      "[2,   600] loss: 0.579\n",
      "[2,  1200] loss: 0.656\n",
      "[2,  1800] loss: 0.814\n",
      "2.863374948501587 seconds\n",
      "[3,   600] loss: 0.580\n",
      "[3,  1200] loss: 0.654\n",
      "[3,  1800] loss: 0.807\n",
      "2.9041826725006104 seconds\n",
      "[4,   600] loss: 0.581\n",
      "[4,  1200] loss: 0.653\n",
      "[4,  1800] loss: 0.803\n",
      "2.9221177101135254 seconds\n",
      "[5,   600] loss: 0.581\n",
      "[5,  1200] loss: 0.652\n",
      "[5,  1800] loss: 0.799\n",
      "2.899216651916504 seconds\n",
      "[6,   600] loss: 0.582\n",
      "[6,  1200] loss: 0.651\n",
      "[6,  1800] loss: 0.796\n",
      "2.901210069656372 seconds\n",
      "[7,   600] loss: 0.582\n",
      "[7,  1200] loss: 0.650\n",
      "[7,  1800] loss: 0.792\n",
      "2.881296396255493 seconds\n",
      "[8,   600] loss: 0.583\n",
      "[8,  1200] loss: 0.649\n",
      "[8,  1800] loss: 0.789\n",
      "2.8633663654327393 seconds\n",
      "[9,   600] loss: 0.583\n",
      "[9,  1200] loss: 0.648\n",
      "[9,  1800] loss: 0.785\n",
      "2.8564069271087646 seconds\n",
      "[10,   600] loss: 0.584\n",
      "[10,  1200] loss: 0.647\n",
      "[10,  1800] loss: 0.782\n",
      "2.886274814605713 seconds\n",
      "[11,   600] loss: 0.584\n",
      "[11,  1200] loss: 0.646\n",
      "[11,  1800] loss: 0.779\n",
      "2.864372491836548 seconds\n",
      "[12,   600] loss: 0.585\n",
      "[12,  1200] loss: 0.645\n",
      "[12,  1800] loss: 0.776\n",
      "2.8812978267669678 seconds\n",
      "[13,   600] loss: 0.585\n",
      "[13,  1200] loss: 0.644\n",
      "[13,  1800] loss: 0.773\n",
      "2.8793063163757324 seconds\n",
      "[14,   600] loss: 0.585\n",
      "[14,  1200] loss: 0.643\n",
      "[14,  1800] loss: 0.770\n",
      "2.8633768558502197 seconds\n",
      "[15,   600] loss: 0.586\n",
      "[15,  1200] loss: 0.643\n",
      "[15,  1800] loss: 0.767\n",
      "2.869349718093872 seconds\n",
      "[16,   600] loss: 0.586\n",
      "[16,  1200] loss: 0.642\n",
      "[16,  1800] loss: 0.764\n",
      "2.8653666973114014 seconds\n",
      "[17,   600] loss: 0.586\n",
      "[17,  1200] loss: 0.641\n",
      "[17,  1800] loss: 0.761\n",
      "2.8683536052703857 seconds\n",
      "[18,   600] loss: 0.586\n",
      "[18,  1200] loss: 0.640\n",
      "[18,  1800] loss: 0.758\n",
      "2.8643739223480225 seconds\n",
      "[19,   600] loss: 0.587\n",
      "[19,  1200] loss: 0.639\n",
      "[19,  1800] loss: 0.755\n",
      "2.849436044692993 seconds\n",
      "[20,   600] loss: 0.587\n",
      "[20,  1200] loss: 0.638\n",
      "[20,  1800] loss: 0.752\n",
      "2.861387014389038 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#train the network for 20 epochs\n",
    "\n",
    "for epoch in range(20): # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print('{} seconds'.format(time.time() - t0))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the network parameters\n",
    "\n",
    "PATH = './params/seismic_net_10.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted non fault and fault probabilities\n",
      "tensor([[0.4707, 0.5293],\n",
      "        [0.4676, 0.5324],\n",
      "        [0.4224, 0.5776],\n",
      "        [0.3885, 0.6115],\n",
      "        [0.3749, 0.6251],\n",
      "        [0.3592, 0.6408],\n",
      "        [0.4470, 0.5530],\n",
      "        [0.3738, 0.6262],\n",
      "        [0.3772, 0.6228],\n",
      "        [0.4009, 0.5991]], grad_fn=<SoftmaxBackward>)\n",
      "Predicted:  fault\n"
     ]
    }
   ],
   "source": [
    "#test sample data from testing\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "test_ips, labels = dataiter.next()\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(test_ips)\n",
    "print('predicted non fault and fault probabilities')\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test dataset: 56 %\n",
      "6000.0\n"
     ]
    }
   ],
   "source": [
    "#test the performance of the network on whole dataset\n",
    "\n",
    "correct = 0.00\n",
    "total = 0.00\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        seis_data, labels = data\n",
    "        outputs = net(seis_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test dataset: %d %%' % (\n",
    "    100 * correct / total))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "59.376666666666665\n"
     ]
    }
   ],
   "source": [
    "#this part runs the code on GPU 10 times and reports the average accuracy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(100, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 8)\n",
    "            self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_10.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()[0].to(device), dataiter.next()[1].to(device)\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    net.to(device)\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "60.151666666666664\n"
     ]
    }
   ],
   "source": [
    "#this part is same code as above (runs 10 times and reports avg accuracy) but doesn't need GPU to run\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(100, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 8)\n",
    "            self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_10.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
