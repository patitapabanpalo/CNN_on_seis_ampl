{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "import os\n",
    "import numpy\n",
    "import time\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of training dataset and its labels\n",
      "torch.Size([20000, 23, 23])\n",
      "torch.Size([20000])\n",
      "the size of testing dataset and its labels\n",
      "torch.Size([6000, 23, 23])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "#the following codes reads the csv data files and converts them to trainloader and testloader\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "TrainList = []\n",
    "LabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Point\\Training\\Fault',\"23\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 23 and len(your_list[0]) == 23):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Point\\Training\\Non-Fault',\"23\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 23 and len(your_list[0]) == 23):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TrainList.append(your_list)\n",
    "                    LabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "inputs = torch.FloatTensor(TrainList)\n",
    "#labellist = [0,0,1,1]\n",
    "labels = torch.tensor(LabelList, dtype=torch.long)\n",
    "train_data = []\n",
    "for i in range(len(inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    train_data.append([inputs[i], labels[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=10)\n",
    "print('the size of training dataset and its labels')\n",
    "print(inputs.shape);\n",
    "print(labels.shape);\n",
    "\n",
    "TestList = []\n",
    "OLabelList = []\n",
    "\n",
    "directory = os.path.join(r'Data_Point\\Testing\\Fault',\"23\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 23 and len(your_list[0]) == 23):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(1)\n",
    "            os.chdir(original_directory)\n",
    "            \n",
    "directory = os.path.join(r'Data_Point\\Testing\\Non-Fault',\"23\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.chdir(directory)\n",
    "            with open(file, 'r') as f:\n",
    "                reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                your_list = list(reader)\n",
    "                if (len(your_list) == 23 and len(your_list[0]) == 23):\n",
    "                    #your_list = normalise_list(your_list,10)\n",
    "                    TestList.append(your_list)\n",
    "                    OLabelList.append(0)\n",
    "            os.chdir(original_directory)\n",
    "\n",
    "#print(TrainList[0])\n",
    "test_inputs = torch.FloatTensor(TestList)\n",
    "#labellist = [0,0,1,1]\n",
    "test_labels = torch.tensor(OLabelList, dtype=torch.long)\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(test_inputs)):\n",
    "#    print(csvlabels[i])\n",
    "    test_data.append([test_inputs[i], test_labels[i]])\n",
    "    \n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=10)\n",
    "print('the size of testing dataset and its labels')\n",
    "print(test_inputs.shape);\n",
    "print(test_labels.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two calsses of data for the labels 0 and 1 respectively \n",
    "\n",
    "classes = ('non_fault', 'fault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26c5a765b88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an seismic data\n",
    "print('three random examples of training fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[1200], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[1600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[8400], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three random examples of training non-fault data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26c5a88e408>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('three random examples of training non-fault data')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputs[10800], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(inputs[14300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(inputs[17700], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four random examples of testing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26c5a9bf608>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABqCAYAAAClIwp2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAM60lEQVR4nO3df4xdZZ3H8fe305Z22DL9MS2WtnQKFpW2SaVXXIIQE/xBiIZ1E5TGuFUgJbVk18Q1NutGnWyyK9Q1bBTRxm3sGkVIhNBtFhQbsGpcnSl2+WEFWqQwSy1TtpTSMkyHfvePc+fOmeG2c+fcc547z72fV9LMc8/cO893Pj359vTc555j7o6IiMRnSqMLEBGRbNTARUQipQYuIhIpNXARkUipgYuIREoNXEQkUnU1cDO72syeMrN9ZrYpr6IkoXyLo2yLo2zDsazrwM2sDXga+CDQB/QAa939D/mV17qUb3GUbXGUbVhT63jtpcA+d38WwMx+DFwLnPYvqnP6dO+aObOOKQsyiWrafejQYXefzwTznTKl09vauoLVCTA0NPKP/1lnWWU8Y8bIc44eHfWKyqi9ffSud+JE3tVVsztTtgCdZ5/tXbNnhygySs+98gqHjx83smQ7Z453LVoUptBI7X7yyeF9d5R6Gvgi4IXU4z7gvWd6QdfMmfRefnkdUxZk5cpGV1BhmzcfKA8nlG9bWxdz5vQWWdpb9PefrIzPP39aZfzOd448Z8eOkbH7S5XxihULRv2s3gClu1umbAG6Zs+m9+abiyoteqXvfnd4OPFsFy2i9957C6qsOdg73nGg2vZ6zoFblW1vOR9jZuvNrNfMevsHB+uYruWMm28621On+gOV1RQmvu8ePx6grKYw8WyPHAlQVnOq5wi8D1iSerwYeHHsk9x9C7AFoNTRoQuv1G7cfNPZLltW8u7ucMUB9PWNHHWn/xNTKo2Mf/7zkfEf/zhy1H399aN/1urVeVd3RhPed5cuLfm3O78cproI9U/dPjyccLZLlpT8GzsuKrrEplTPEXgPsNzMlpnZdOB6YPs4r5HaKd/iKNviKNuAMh+Bu/uQmd0C/BRoA7a6+5O5VdbilG9xlG1xlG1YmZcRZprMVjncF2y+WvkXtjS6hArbvHm3u5fGf+Zol5j5r4oo6AzaZ82qjE8cO1YZp5cbvO0042mMWp6C2Tn5FleFu2XKFuA8M1+fd0FNZAvwonu189/jUrbj64aq+64+iSkiEik1cBGRSKmBi4hESg1cRCRSauAiIpFSAxcRiVQ9n8RsGrb5w40uIWVzowsI4j3vGb1scN684ud88MHsrz2vo4OvXnllfsU0mR27dmV+7Wuz1rCrFPY6PtF5uPoKTR2Bi4hESg1cRCRSOoXSJKYsXEj7TTeFnbSrqzJs37evMi498MDIcy67bGS8YkVl+L0rRv+oEJfaXrq0+DlEQtIRuIhIpNTARUQiFfgUyhEm48Ws4JJGF9AShm65ZWTMLaO+99LYJ4vIuHQELiISKTVwEZFIqYGLiERKDVxEJFJq4CIikQq8CmUQOBB2yppMxppkUpk/H26+udFVTF5792Z+6eLF8PWv51hLE1qzpvp2HYGLiERKDVxEJFJq4CIikQp6Dnwh/aznjpBT1qSbjY0uoW67D87D/ukzgWc9nBpfWxnNmPHPlfHAnkcrY//WyMWsLrn77tE/amAg9+reoqen+DlEAhr3CNzMtprZS2b2RGrbXDN7yMyeKX+dU2yZzWwnsBW4q7JF+ebjhj/9iQW//z0rn6jsuso2JzfcfjsLPvlJVn72s5Vtyja8Wk6hfB+4esy2TcBOd19O0oE25VxXC3kX8NGxG5VvDj7d2cmDF100drOyzcGnP/ABHuzuHrtZ2QY27ikUd99lZl1jNl8LvL883gY8Anwxx7oCa/QywqPAyXQdTZOvb/he6tGK0z6vCFfOmsVzb7wxdnPTZNtIV65cyXOHDo3drGwDy/om5rnufhCg/HXB6Z5oZuvNrNfMek9knKwF1ZRvOlv4v6AFRizTvtt/9GiwAiOWKdsjR/qDFdhsCl+F4u5b3L3k7qX2oidrMelsYW6jy2k66Xznd3Q0upymks52zpz5jS4nWllXoRwys4XuftDMFhL55ZzvYEdD538ZuBP4R3YMr4dpqnwnGWVbnEzZDu7dzfNrqt91Xc4s6xH4dmBdebwOuD+fcqRM+RZH2RZH2QY27hG4md1F8sZEp5n1AV8BvgbcY2Y3As8D1xVZZDPbCjwDvAZ8KdnUifLNxdr9+3nk2DEODw2xeM8eULa5WXvbbTzy+OMcfvVVFq9bx9S2NlC2wdWyCmXtab51Vc61tKQbxjzeCIfd/WWUb93uuvDCUY+tpydztrv39WMf+U5epTWBc4DLAfjflwF+ifbb8PRRehGRSKmBi4hEKvD1wKU4rwIPBJ7zSGp8QWU0MDCvMrY7b0o9J/3Bj9Erf8zOybe0qrTSQZqLjsBFRCKlBi4iEqmgp1AOspJu7gs5ZU3uYHmjS2gqM2Z8qDIeGHj0DM+Mx5oFZ9H7iWWNLmPSKt39u8yv3U8HH+OKHKtpRtU/bKgjcBGRSKmBi4hESg1cRCRSWkbYNAaBvsBzppcRzgg8t4joCFxEJFJq4CIikQp6CqW9/SxWrHh7yClrsrHnp40uIeXDmV51AX/mVv4l51rOrDM1fiU1Xsvng9Yh0qp0BC4iEik1cBGRSGkViuRuYOBnqUe/To3Td9hKn3QB9xAXs8pu90tvYN98vtFlTGJvNLqAlqQjcBGRSKmBi4hEKugplBMnjtHTszPklDWZNu1D4z8pkJMnG11BNh/jR40uQSK1evXb+cUv/rPRZUxqHR3Vr2WvI3ARkUipgYuIREqrUCQnL6bGr6fG6ZUbh1PjA2NeP7lXoUhx2l5/jXP27Gp0GVEat4Gb2RLgP4C3AaeALe7+b2Y2F7gb6AKeAz7u7kdO93Pkrdxf4M03/wb3PwNTmDJlPQDKNi9HgG0k9ws1gAWgfPPxOvAoMECSbfLmjbINq5ZTKEPA5939XcBfAhvN7GJgE7DT3ZcDO8uPZUKm0tb2r0ybtpepU/+bU6fugOSyfso2F1OAvwa+DHwBYIH23bwYsAK4CrgCGETZhjfuEbi7HwQOlsfHzGwvsAi4Fnh/+WnbgEeALxZSZcFOntza4Ar2lL+eDTCdCLO9j7+vjPektqevl3JeavzYaW4RVaTu5LCxqfbdxpnByCWEp5H8Y3lK2QY2oTcxzawLeDfwW+DccnMfbvIL8i6utRymfL74NZRt7sqf+2xH+24BTgBvgrINruYGbmZ/AfwE+Jy7vzqB1603s14z6x378WkZNgB8G7gekvcZapLOtua/kBY0CNyTDF/Ivu8OFlRd7IaA3wEzyZpt/yvqC1nV1MDNbBpJ8/6hu99b3nzIzBaWv7+Q0Re6qHD3Le5ecvcSzM6j5iYzRNK83wusGd444Wy1hqO6N0ma96rk4XCnyLDvTi++2OicImnei0lOowAZsp0/W30hq1pWoRjw78Bed/9G6lvbgXXA18pf7y+kwqbmwPeBhYy5DviEs32WlVzH9vxLPKORZYH3cWngucfnJEF2ApcBqUtsTTjfNRedS++df1tEmVFyd9bdeitzZ13B7Rs3UtqwYfhb6gsB1bIO/HLgU8DjZjb8/tQ/kPwF3WNmN5KcvL2umBKb2T7gNyRHMF8d3tiBss3FC8BjJCdhv5NsutjMrkH51u3XTzzBDx56iFXLlrF6/Xqe7utD2YZXyyqUX1FeRFvFVfmW02qWk/znJu3Go+7+Msq2bucDX0k97oY/uPt/lR8q3zq8b9UqfOfIdY1KGzbQ+9RTyjYwfRJTMns4ddpEb0OJhKdroYiIREoNXEQkUubu4SYz6weOM/qqRq2mkzP//kvdff5Ef2g52wM1/PxmVki2oH0XZVu0TPkGbeAAZtabrKttTUX//q2cr7ItjrItVtbfX6dQREQipQYuIhKpRjTwLQ2YczIp+vdv5XyVbXGUbbEy/f7Bz4GLiEg+dApFRCRSQRu4mV1tZk+Z2T4za+o7dZjZEjN72Mz2mtmTZvZ35e1zzewhM3um/HVOTvMp24KyLf9s5at9t265Z+vuQf4AbcB+4AKSa3P+D3BxqPlD/yG5xOAl5fEs4GngYuA2YFN5+ybgVmU7ebNVvtp3J3O2IY/ALwX2ufuz7j4I/Jjk9ktNyd0Puvuj5fExIH0rum3lp20D/iqH6ZRtcdmC8tW+m5O8sw3ZwBeRXOFzWF95W9MLcCs6ZVvs7byUr/bd3OWRbcgGXu2StE2/BCbrregmOk2Vbco2x6mqbFO+OU1TZZuyrVHIBt4HLEk9Xgy8GHD+4Oq5Fd0EKdtEEdmC8tW+m6M8sw3ZwHuA5Wa2zMymk9zBN/Q9wIKp4VZ0kN8tp5RtoohsQfkO075bp9yzDfwO7DUk77ruB77U6HeEC/5d30fyX8HHgD3lP9cA84CdwDPlr3OV7eTOVvlq352s2eqTmCIikdInMUVEIqUGLiISKTVwEZFIqYGLiERKDVxEJFJq4CIikVIDFxGJlBq4iEik/h8uuaqu6jfcVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('four random examples of testing data')\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(test_inputs[450], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(test_inputs[2300], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(test_inputs[4600], cmap=plt.cm.seismic)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(test_inputs[5800], cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "        self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(529, 640)\n",
    "        self.fc2 = nn.Linear(640, 256)\n",
    "        self.fc3 = nn.Linear(256, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "#optimizer = optim.Adadelta(net.parameters(), lr=0.0001, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "#optimizer = optim.AdamW(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "#optimizer = optim.ASGD(net.parameters(), lr=0.0001, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#optimizer = optim.Rprop(net.parameters(), lr=0.0001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   600] loss: 0.683\n",
      "[1,  1200] loss: 0.670\n",
      "[1,  1800] loss: 0.658\n",
      "16.760112524032593 seconds\n",
      "[2,   600] loss: 0.639\n",
      "[2,  1200] loss: 0.623\n",
      "[2,  1800] loss: 0.610\n",
      "17.17828345298767 seconds\n",
      "[3,   600] loss: 0.590\n",
      "[3,  1200] loss: 0.574\n",
      "[3,  1800] loss: 0.559\n",
      "17.189224243164062 seconds\n",
      "[4,   600] loss: 0.542\n",
      "[4,  1200] loss: 0.531\n",
      "[4,  1800] loss: 0.519\n",
      "17.208142280578613 seconds\n",
      "[5,   600] loss: 0.510\n",
      "[5,  1200] loss: 0.494\n",
      "[5,  1800] loss: 0.493\n",
      "17.10957670211792 seconds\n",
      "[6,   600] loss: 0.484\n",
      "[6,  1200] loss: 0.476\n",
      "[6,  1800] loss: 0.478\n",
      "17.097633361816406 seconds\n",
      "[7,   600] loss: 0.466\n",
      "[7,  1200] loss: 0.461\n",
      "[7,  1800] loss: 0.464\n",
      "17.167320251464844 seconds\n",
      "[8,   600] loss: 0.455\n",
      "[8,  1200] loss: 0.454\n",
      "[8,  1800] loss: 0.450\n",
      "17.062782287597656 seconds\n",
      "[9,   600] loss: 0.442\n",
      "[9,  1200] loss: 0.447\n",
      "[9,  1800] loss: 0.444\n",
      "16.973170280456543 seconds\n",
      "[10,   600] loss: 0.439\n",
      "[10,  1200] loss: 0.437\n",
      "[10,  1800] loss: 0.436\n",
      "16.93136191368103 seconds\n",
      "[11,   600] loss: 0.431\n",
      "[11,  1200] loss: 0.432\n",
      "[11,  1800] loss: 0.433\n",
      "16.942314624786377 seconds\n",
      "[12,   600] loss: 0.427\n",
      "[12,  1200] loss: 0.428\n",
      "[12,  1800] loss: 0.425\n",
      "17.05979633331299 seconds\n",
      "[13,   600] loss: 0.426\n",
      "[13,  1200] loss: 0.422\n",
      "[13,  1800] loss: 0.418\n",
      "17.096630573272705 seconds\n",
      "[14,   600] loss: 0.419\n",
      "[14,  1200] loss: 0.418\n",
      "[14,  1800] loss: 0.415\n",
      "17.05481719970703 seconds\n",
      "[15,   600] loss: 0.415\n",
      "[15,  1200] loss: 0.418\n",
      "[15,  1800] loss: 0.411\n",
      "17.074729442596436 seconds\n",
      "[16,   600] loss: 0.412\n",
      "[16,  1200] loss: 0.409\n",
      "[16,  1800] loss: 0.408\n",
      "16.426586627960205 seconds\n",
      "[17,   600] loss: 0.411\n",
      "[17,  1200] loss: 0.407\n",
      "[17,  1800] loss: 0.406\n",
      "16.07115077972412 seconds\n",
      "[18,   600] loss: 0.406\n",
      "[18,  1200] loss: 0.406\n",
      "[18,  1800] loss: 0.404\n",
      "16.14084506034851 seconds\n",
      "[19,   600] loss: 0.404\n",
      "[19,  1200] loss: 0.399\n",
      "[19,  1800] loss: 0.402\n",
      "16.12690782546997 seconds\n",
      "[20,   600] loss: 0.401\n",
      "[20,  1200] loss: 0.400\n",
      "[20,  1800] loss: 0.397\n",
      "16.130889415740967 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#train the network for 20 epochs\n",
    "\n",
    "for epoch in range(20): # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print('{} seconds'.format(time.time() - t0))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the network parameters\n",
    "\n",
    "PATH = './params/seismic_net_23.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted non fault and fault probabilities\n",
      "tensor([[0.4729, 0.5271],\n",
      "        [0.9310, 0.0690],\n",
      "        [0.9366, 0.0634],\n",
      "        [0.0605, 0.9395],\n",
      "        [0.3825, 0.6175],\n",
      "        [0.9490, 0.0510],\n",
      "        [0.0327, 0.9673],\n",
      "        [0.9783, 0.0217],\n",
      "        [0.9380, 0.0620],\n",
      "        [0.9874, 0.0126]], grad_fn=<SoftmaxBackward>)\n",
      "Predicted:  fault\n"
     ]
    }
   ],
   "source": [
    "#test sample data from testing\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "test_ips, labels = dataiter.next()\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(test_ips)\n",
    "print('predicted non fault and fault probabilities')\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test dataset: 90 %\n",
      "6000.0\n"
     ]
    }
   ],
   "source": [
    "#test the performance of the network on whole dataset\n",
    "\n",
    "correct = 0.00\n",
    "total = 0.00\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        seis_data, labels = data\n",
    "        outputs = net(seis_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test dataset: %d %%' % (\n",
    "    100 * correct / total))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "89.77000000000001\n"
     ]
    }
   ],
   "source": [
    "#this part runs the code on GPU 10 times and reports the average accuracy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(529, 640)\n",
    "            self.fc2 = nn.Linear(640, 256)\n",
    "            self.fc3 = nn.Linear(256, 32)\n",
    "            self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_23.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()[0].to(device), dataiter.next()[1].to(device)\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    net.to(device)\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for 1\n",
      "Finished Training for 2\n",
      "Finished Training for 3\n",
      "Finished Training for 4\n",
      "Finished Training for 5\n",
      "Finished Training for 6\n",
      "Finished Training for 7\n",
      "Finished Training for 8\n",
      "Finished Training for 9\n",
      "Finished Training for 10\n",
      "the avg acc is \n",
      "89.87\n"
     ]
    }
   ],
   "source": [
    "#this part is same code as above (runs 10 times and reports avg accuracy) but doesn't need GPU to run\n",
    "\n",
    "acc_list = []\n",
    "for j in range(10):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "            self.conv1 = nn.Conv2d(6, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv1 = nn.Conv2d(16, 25, 3)\n",
    "            self.conv1 = nn.Conv2d(25, 32, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(529, 640)\n",
    "            self.fc2 = nn.Linear(640, 256)\n",
    "            self.fc3 = nn.Linear(256, 32)\n",
    "            self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            return x\n",
    "    \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.0001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    \n",
    "    for epoch in range(20): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print('Finished Training for %d'% (j+1))\n",
    "    \n",
    "    PATH = './seismic_net_23.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    dataiter = iter(testloader)\n",
    "    test_ips, labels = dataiter.next()\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    correct = 0.00\n",
    "    total = 0.00\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            seis_data, labels = data\n",
    "            outputs = net(seis_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accur = 100*(correct/total)\n",
    "    acc_list.append(accur)\n",
    "avg_acc = statistics.mean(acc_list)\n",
    "print (\"the avg acc is \")\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
